{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vz0Hy4KPZhJC",
    "outputId": "1758b7d0-2f35-4ad8-cb3f-8b8c426db8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.2.2-py2.py3-none-any.whl (80 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |████                            | 10 kB 28.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 20 kB 29.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 30 kB 17.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 40 kB 15.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 51 kB 7.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 61 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 71 kB 7.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 80 kB 4.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.1.5)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.19.5)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n",
      "\n",
      "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler,LabelEncoder,PowerTransformer\n",
    "from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error\n",
    "from sklearn.impute import SimpleImputer,KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import f_regression,RFECV\n",
    "from sklearn.linear_model import ElasticNet,Ridge,Lasso\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn.linear_model import LinearRegression\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import lightgbm as lgm\n",
    "! pip install category_encoders\n",
    "from category_encoders import TargetEncoder\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import scipy as sc\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "import lightgbm as lgm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "j_sIALNEZpuw"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "qynZbnc7Z7Ts"
   },
   "outputs": [],
   "source": [
    "mean_gridlivarea=train.loc[train['GrLivArea']>4500,\"SalePrice\"].mean()\n",
    "mean_masvnrarea=train.loc[train['MasVnrArea']>1200,\"SalePrice\"].mean()\n",
    "mean_bsmtfin2=train.loc[train['BsmtFinSF2']>1400,\"SalePrice\"].mean()\n",
    "mean_woodeck=train.loc[train['WoodDeckSF']>800,\"SalePrice\"].mean()\n",
    "mean_openporch=train.loc[train['OpenPorchSF']>500,\"SalePrice\"].mean()\n",
    "mean_enclosedporch=train.loc[train['EnclosedPorch']>500,\"SalePrice\"].mean()\n",
    "outlier_indices=[934,1298,249,313,335,706,205,55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "KjOpujrDaA5I"
   },
   "outputs": [],
   "source": [
    "def GridSearch(model,param_grid,X,y):\n",
    "    grid=GridSearchCV(model,param_grid,cv=10,scoring='neg_mean_squared_error')\n",
    "    grid.fit(X,y)\n",
    "    print(np.sqrt(-grid.best_score_))\n",
    "    print(grid.best_params_)\n",
    "def cross_validate(model,X,y):\n",
    "    scores=cross_val_score(model,X,y,scoring='neg_mean_squared_error')\n",
    "    print(np.sqrt(-scores.mean()))\n",
    "def create_submission(predictions,name):\n",
    "    submission=pd.DataFrame()\n",
    "    submission['Id']=test_idx\n",
    "    submission['SalePrice']=predictions\n",
    "    submission.to_csv(name+\".csv\",index=False)\n",
    "def Numerical_Summary(dataframe):\n",
    "    summary=pd.DataFrame()\n",
    "    for col in dataframe.columns:\n",
    "        if dataframe[col].dtype==object:\n",
    "            continue\n",
    "        row={\"Column\":col,\"Missing_Values\":dataframe[col].isnull().sum(),\"Variance\":dataframe[col].var(),\"Skewness\":dataframe[col].skew()}\n",
    "        summary=summary.append(pd.Series(row),ignore_index=True)\n",
    "    \n",
    "    return summary\n",
    "def categorical_summary(dataframe):\n",
    "    print(\"Categorical Features Summary:-\")\n",
    "    print(\"=\"*50)\n",
    "    for col in dataframe.columns:\n",
    "        if dataframe[col].dtype!=object:\n",
    "            continue\n",
    "        print(\"Column =\",col)\n",
    "        print(\"Value Counts:-\",dataframe[col].value_counts())\n",
    "        print(\"Missing Values=\",dataframe[col].isnull().sum())\n",
    "        print(\"=\"*50)\n",
    "numerical_columns=[col for col in train.columns if train[col].dtype!=object and col!='SalePrice']\n",
    "categorical_columns=[col for col in train.columns if train[col].dtype==object]\n",
    "#outlier_indices=[934,249,313,335,706,1298]\n",
    "skewed_features=[col for col in train.columns if col not in categorical_columns and abs(train[col].skew())>0.5 and col!='SalePrice'  ]\n",
    "def scatterplots():\n",
    "    fig,ax=plt.subplots(9,2,figsize=(30,30))\n",
    "    sns.scatterplot(train['LotFrontage'],train['SalePrice'],ax=ax[0,0])\n",
    "    sns.scatterplot(train['LotArea'],train['SalePrice'],ax=ax[0,1])\n",
    "    sns.scatterplot(train['MasVnrArea'],train['SalePrice'],ax=ax[1,0])\n",
    "    sns.scatterplot(train['GrLivArea'],train['SalePrice'],ax=ax[1,1])\n",
    "    sns.scatterplot(train['BsmtFinSF1'],train['SalePrice'],ax=ax[2,0])\n",
    "    sns.scatterplot(train['BsmtFinSF2'],train['SalePrice'],ax=ax[2,1])\n",
    "    sns.scatterplot(train['1stFlrSF'],train['SalePrice'],ax=ax[3,0])\n",
    "    sns.scatterplot(train['2ndFlrSF'],train['SalePrice'],ax=ax[3,1])\n",
    "    sns.scatterplot(train['GarageArea'],train['SalePrice'],ax=ax[4,0])\n",
    "    sns.scatterplot(train['TotalBsmtSF'],train['SalePrice'],ax=ax[4,1])\n",
    "    sns.scatterplot(train['YearBuilt'],train['SalePrice'],ax=ax[5,0])\n",
    "    sns.scatterplot(train['BsmtUnfSF'],train['SalePrice'],ax=ax[5,1])\n",
    "    sns.scatterplot(train['LowQualFinSF'],train['SalePrice'],ax=ax[6,0])\n",
    "    sns.scatterplot(train['GarageArea'],train['SalePrice'],ax=ax[6,1])\n",
    "    sns.scatterplot(train['WoodDeckSF'],train['SalePrice'],ax=ax[7,0])\n",
    "    sns.scatterplot(train['OpenPorchSF'],train['SalePrice'],ax=ax[7,1])\n",
    "    sns.scatterplot(train['EnclosedPorch'],train['SalePrice'],ax=ax[8,0])\n",
    "    sns.scatterplot(train['3SsnPorch'],train['SalePrice'],ax=ax[8,1])\n",
    "    \n",
    "    \n",
    "def Impute_Missing(dataframe):\n",
    "    df=dataframe.copy()\n",
    "    for col in [\"Alley\",'MasVnrType','PoolQC','Fence', 'MiscFeature',\"FireplaceQu\",'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','GarageType','GarageQual',\n",
    "       'GarageCond','GarageFinish','BsmtQual']:\n",
    "        df[col].fillna(\"Missing\",inplace=True)\n",
    "    for col in ['MSZoning','Exterior1st','Exterior2nd','Electrical','KitchenQual','Functional',\"SaleType\",\"Utilities\"]:\n",
    "        df[col].fillna(df[col].mode()[0],inplace=True)\n",
    "    for col in ['MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageYrBlt','BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea']:\n",
    "        df[col].fillna(0,inplace=True)\n",
    "    df['LotFrontage']=dataframe.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "    return df\n",
    "def Heatmap(dataframe):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.heatmap(dataframe,annot=True)\n",
    "def Fix_Skew(dataframe):\n",
    "    df=dataframe.copy()\n",
    "    for col in skewed_features:\n",
    "        df[col]=boxcox1p(df[col],0.15)\n",
    "    return df\n",
    "def Feature_Engineering(dataframe):\n",
    "    all_data=dataframe.copy()\n",
    "    all_data['Total_Living_Sq_Area']=all_data['BsmtFinSF1']+all_data['BsmtFinSF2']+all_data['1stFlrSF']+all_data['2ndFlrSF']\n",
    "    all_data['Total_House_Area']=all_data['TotalBsmtSF']+all_data['1stFlrSF']+all_data['2ndFlrSF']\n",
    "    all_data['Total_Bathrooms']=all_data['BsmtFullBath']+0.5*all_data['BsmtHalfBath']+0.5*all_data['HalfBath']+all_data['FullBath']\n",
    "    all_data['Total_Porch']=all_data['WoodDeckSF']+all_data['OpenPorchSF']+all_data['EnclosedPorch']+all_data['3SsnPorch']+all_data['ScreenPorch']\n",
    "    all_data['Has_Pool']=all_data['PoolQC'].apply(lambda x: 1 if x is np.nan else 0)\n",
    "    all_data['Has_Alley']=all_data['Alley'].apply(lambda x: 1 if x is  np.nan else 0)\n",
    "    all_data['Has_Fireplace']=all_data['FireplaceQu'].apply(lambda x: 1 if x is  np.nan else 0)\n",
    "    all_data['Age_House']=all_data['YrSold']-all_data['YearBuilt']\n",
    "    all_data['Years_After_Remodal']=all_data['YrSold']-all_data['YearRemodAdd']\n",
    "    return all_data\n",
    "test_idx=test['Id'].copy()\n",
    "def remove_id(dataframe):\n",
    "    dataframe.drop(\"Id\",axis=1,inplace=True)\n",
    "def create_submission(predictions,name):\n",
    "    df=pd.DataFrame()\n",
    "    df['Id']=test_idx\n",
    "    df['SalePrice']=predictions\n",
    "    df.to_csv(name+\".csv\",index=False)\n",
    "def create_target(dataframe):\n",
    "    dataframe['SalePrice']=np.log1p(dataframe['SalePrice'])\n",
    "    y=dataframe['SalePrice'].copy()\n",
    "    dataframe.drop(\"SalePrice\",axis=1,inplace=True)\n",
    "    return y\n",
    "def feature_selection(dataframe,ncols):\n",
    "    targetencoder=TargetEncoder(cols=categorical_columns)\n",
    "    train_data=dataframe[numerical_columns].join(targetencoder.fit_transform(dataframe[categorical_columns],dataframe['SalePrice']))\n",
    "    dtree=ExtraTreesRegressor()\n",
    "    dtree.fit(train_data,dataframe[\"SalePrice\"])\n",
    "    feature_importances=pd.DataFrame(sorted(zip(dtree.feature_importances_,train_data),reverse=True),columns=['Importance',\"Feature\"])\n",
    "    cols=list(feature_importances.loc[:ncols,\"Feature\"])\n",
    "    return cols\n",
    "def feature_selection_RFE(dataframe,cols):\n",
    "    rfe=RFE(ExtraTreesRegressor(),n_features_to_select=cols)\n",
    "    targetencoder=TargetEncoder(cols=categorical_columns)\n",
    "    train_data=dataframe[numerical_columns].join(targetencoder.fit_transform(dataframe[categorical_columns],dataframe['SalePrice']))\n",
    "    rfe.fit(train_data,dataframe['SalePrice'])\n",
    "    features_selected=pd.DataFrame(zip(rfe.support_,train_data.columns),columns=[\"Is_Selected\",\"Column\"])\n",
    "    feature_selected=features_selected[features_selected['Is_Selected']==True]['Column']\n",
    "    return list(feature_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHHnzysraFIW",
    "outputId": "dfdfe270-faba-4a36-e894-c85e3589256e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING OUTLIER ROWS DONE\n",
      "==================================================\n",
      "MISSING VALUES IMPUTATION  DONE\n",
      "==================================================\n",
      "FIXING SKEW FEATURES DONE\n",
      "==================================================\n",
      "FEATURE ENGINEERING DONE\n",
      "==================================================\n",
      "FEATURE SELECTION DONE\n",
      "==================================================\n",
      "DATA IS READY FOR MODELLING\n",
      "==================================================\n",
      "Training Data Shape after preprocessing for modelling (1451, 60)\n",
      "Testing Data Shape after preprocessing for modelling (1459, 60)\n"
     ]
    }
   ],
   "source": [
    "# Removing Outlier Rows\n",
    "train=train.drop(index=outlier_indices).reset_index().drop(\"index\",axis=1)\n",
    "#mean=train\n",
    "train=train[train['GrLivArea']<4500].reset_index(drop=True)\n",
    "print(\"REMOVING OUTLIER ROWS DONE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Missing Values Imputation\n",
    "train=Impute_Missing(train)\n",
    "test=Impute_Missing(test)\n",
    "print(\"MISSING VALUES IMPUTATION  DONE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Removing Skew \n",
    "train=Fix_Skew(train)\n",
    "test=Fix_Skew(test)\n",
    "print(\"FIXING SKEW FEATURES DONE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Feature Engineering\n",
    "train=Feature_Engineering(train)\n",
    "test=Feature_Engineering(test)\n",
    "print(\"FEATURE ENGINEERING DONE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Feature_selection\n",
    "cols=feature_selection(train,59)\n",
    "train=train[cols+[\"SalePrice\"]].copy()\n",
    "test=test[cols].copy()\n",
    "print(\"FEATURE SELECTION DONE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Creating Target Variable\n",
    "y=create_target(train)\n",
    "new_cat_cols=[col for col in cols if train[col].dtype==object]\n",
    "new_numerical_cols=[col for col in cols if train[col].dtype!=object]\n",
    "\n",
    "# getting dumies\n",
    "final_pipeline=ColumnTransformer(transformers=[(\"Numerical\",RobustScaler(),new_numerical_cols),(\"Categorical\",OneHotEncoder(handle_unknown=\"ignore\"),new_cat_cols)])\n",
    "\n",
    "print(\"DATA IS READY FOR MODELLING\")\n",
    "print(\"=\"*50)\n",
    "print(\"Training Data Shape after preprocessing for modelling\",train.shape)\n",
    "print(\"Testing Data Shape after preprocessing for modelling\",test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFuUaxZVaI-i",
    "outputId": "85b08cca-3f7d-4ab4-bb7e-9f1aa09b3a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1451, 232)\n"
     ]
    }
   ],
   "source": [
    "X=final_pipeline.fit_transform(train)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSAu-2_UAIH0"
   },
   "source": [
    "**Linear Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_BO21PWaSfu",
    "outputId": "6066cfb6-11aa-48bb-86fa-001abd735ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11124056462731058\n",
      "{'alpha': 0.0003}\n"
     ]
    }
   ],
   "source": [
    "GridSearch(Lasso(),{\"alpha\":[0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009]},X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nen_MiDKbvRy",
    "outputId": "5cc7e1a7-7243-43f5-ab03-d2308535107b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11386347660000201\n",
      "{'alpha': 15}\n"
     ]
    }
   ],
   "source": [
    "GridSearch(Ridge(),{\"alpha\":[30,31,32,33,34,35,16,20,15,12]},X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wq_5yKn1AO2k",
    "outputId": "8fcbc1a2-a10c-46be-8fb6-39122d15f109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11085807489455071\n",
      "{'l1_ratio': 0.3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GridSearch(ElasticNet(alpha=0.001),{\"l1_ratio\":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]},X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPIED_9CBOFr",
    "outputId": "b4834051-d14d-473e-f4c8-e45078b50310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation RMSE for lasso = 0.11154276631612817\n"
     ]
    }
   ],
   "source": [
    "lasso_predictions=np.zeros((len(test),1))\n",
    "y_train_pred_lasso=np.zeros((len(train),1))\n",
    "kfold=KFold(random_state=42)\n",
    "for idx,(train_idx,val_idx) in enumerate(kfold.split(train.values,y)):\n",
    "    x_train=final_pipeline.fit_transform(train.loc[train_idx])\n",
    "    x_val=final_pipeline.transform(train.loc[val_idx])\n",
    "    x_test=final_pipeline.transform(test)\n",
    "    lasso=Lasso(alpha=0.0003)\n",
    "    lasso.fit(x_train,y[train_idx])\n",
    "    p=lasso.predict(x_val)\n",
    "    \n",
    "    y_train_pred_lasso[val_idx]+=np.reshape(p,(len(p),1))\n",
    "    p=lasso.predict(x_test)\n",
    "    lasso_predictions+=np.reshape(p,(len(p),1))/5\n",
    "print(\"Cross Validation RMSE for lasso =\",np.sqrt(mean_squared_error(y,y_train_pred_lasso\n",
    "                                                                     )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fQsKPgBBOOx",
    "outputId": "f253bbb7-48c3-42bc-cb8d-36a606fada08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation RMSE for Ridge = 0.11438499948722117\n"
     ]
    }
   ],
   "source": [
    "ridge_predictions=np.zeros((len(test),1))\n",
    "y_train_pred_ridge=np.zeros((len(train),1))\n",
    "kfold=KFold(random_state=42)\n",
    "for idx,(train_idx,val_idx) in enumerate(kfold.split(train.values,y)):\n",
    "  x_train=final_pipeline.fit_transform(train.loc[train_idx])\n",
    "  x_valid=final_pipeline.transform(train.loc[val_idx])\n",
    "  x_test=final_pipeline.transform(test)\n",
    "  ridge=Ridge(alpha=15)\n",
    "  ridge.fit(x_train,y[train_idx])\n",
    "  p=ridge.predict(x_valid)\n",
    "  y_train_pred_ridge[val_idx]+=np.reshape(p,(len(p),1))\n",
    "  p=ridge.predict(x_test)\n",
    "  ridge_predictions+=np.reshape(p,(len(p),1))/5\n",
    "print(\"Cross Validation RMSE for Ridge =\",np.sqrt(mean_squared_error(y,y_train_pred_ridge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MNCBcdHBOWg",
    "outputId": "8ea13a80-e5bb-4bed-998e-b3a080330e2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation RMSE for ElasticNet = 0.11123945637487202\n"
     ]
    }
   ],
   "source": [
    "en_predictions=np.zeros((len(test),1))\n",
    "y_train_pred_en=np.zeros((len(train),1))\n",
    "kfold=KFold(random_state=42)\n",
    "for idx,(train_idx,val_idx) in enumerate(kfold.split(train.values,y)):\n",
    "  x_train=final_pipeline.fit_transform(train.loc[train_idx])\n",
    "  x_valid=final_pipeline.transform(train.loc[val_idx])\n",
    "  x_test=final_pipeline.transform(test)\n",
    "  en=ElasticNet(alpha=0.001,l1_ratio=0.3)\n",
    "  en.fit(x_train,y[train_idx])\n",
    "  p=en.predict(x_valid)\n",
    "  y_train_pred_en[val_idx]+=np.reshape(p,(len(p),1))\n",
    "  p=en.predict(x_test)\n",
    "  en_predictions+=np.reshape(p,(len(p),1))/5\n",
    "print(\"Cross Validation RMSE for ElasticNet =\",np.sqrt(mean_squared_error(y,y_train_pred_en)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd5Tsx7gCf-P"
   },
   "source": [
    "**Boosting Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMefkkMtbvag",
    "outputId": "0ce0ccd0-35f6-478c-d799-47b11f3d91ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11185687687797413\n",
      "{'learning_rate': 0.05, 'n_estimators': 2000}\n"
     ]
    }
   ],
   "source": [
    "GridSearch(GradientBoostingRegressor(max_depth=3,max_features=\"sqrt\",loss='huber'),{'n_estimators':[1000,3000,2000],\"learning_rate\":[0.01,0.05]},X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUOsloJhbvez",
    "outputId": "95fa00e3-fcca-4e24-8f03-6d401c0578ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11533712410051827\n",
      "{'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "GridSearch(XGBRegressor(objective='reg:squarederror',n_estimators=1000,learning_rate=0.05,max_depth=3,gamma=0.01,colsample_bytree=0.8),{'subsample':[0.8,0.9]},X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ImOK4wrgcai",
    "outputId": "94427e4e-4145-4b79-def4-c45c1e780bfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11772169315744874\n",
      "{'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GridSearch(lgm.LGBMRegressor(learning_rate=0.05,n_estimators=750,num_leaves=5,max_depth=3,\n",
    "                            objective=\"regression\",subsample=0.5,colsample_bytree=0.5),{\"max_depth\":[3,5,10,8]},X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIPBT7jbgceA",
    "outputId": "7c4df44e-5b6e-4f5b-8e50-c4a09a1ec667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation RMSE for GradientBoosting = 0.11367758188435981\n"
     ]
    }
   ],
   "source": [
    "gb_predictions=np.zeros((len(test),1))\n",
    "y_train_pred_gb=np.zeros((len(train),1))\n",
    "kfold=KFold(random_state=42)\n",
    "for idx,(train_idx,val_idx) in enumerate(kfold.split(train.values,y)):\n",
    "  x_train=final_pipeline.fit_transform(train.loc[train_idx])\n",
    "  x_valid=final_pipeline.transform(train.loc[val_idx])\n",
    "  x_test=final_pipeline.transform(test)\n",
    "  gb=GradientBoostingRegressor(n_estimators=3000,learning_rate=0.01,loss='huber',max_depth=3,max_features='sqrt')\n",
    "  gb.fit(x_train,y[train_idx])\n",
    "  p=gb.predict(x_valid)\n",
    "  y_train_pred_gb[val_idx]+=np.reshape(p,(len(p),1)) \n",
    "  p=gb.predict(x_test)\n",
    "  gb_predictions+=np.reshape(p,(len(p),1))/5\n",
    "print(\"Cross Validation RMSE for GradientBoosting =\",np.sqrt(mean_squared_error(y,y_train_pred_gb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IyuCi8vjTDxt",
    "outputId": "d61bd80c-9c3c-4f6b-dac4-8d3884c9d296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation RMSE for Xgboost = 0.11782287169291854\n"
     ]
    }
   ],
   "source": [
    "xgboost_predictions=np.zeros((len(test),1))\n",
    "y_train_pred_xgboost=np.zeros((len(train),1))\n",
    "kfold=KFold(random_state=42)\n",
    "for idx,(train_idx,val_idx) in enumerate(kfold.split(train.values,y)):\n",
    "    x_train=final_pipeline.fit_transform(train.loc[train_idx])\n",
    "    x_val=final_pipeline.transform(train.loc[val_idx])\n",
    "    x_test=final_pipeline.transform(test)\n",
    "    xgboost = XGBRegressor(objective='reg:squarederror',n_estimators=1000,learning_rate=0.05,max_depth=3,gamma=0.01,colsample_bytree=0.8,subsample=0.8)\n",
    "    xgboost.fit(x_train,y[train_idx])\n",
    "    p=xgboost.predict(x_val)\n",
    "    \n",
    "    y_train_pred_xgboost[val_idx]+=np.reshape(p,(len(p),1))\n",
    "    p=xgboost.predict(x_test)\n",
    "    xgboost_predictions+=np.reshape(p,(len(p),1))/5\n",
    "print(\"Cross Validation RMSE for Xgboost =\",np.sqrt(mean_squared_error(y,y_train_pred_xgboost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hOv5vUzPaxvK",
    "outputId": "715b3d13-e854-496d-9f0c-cbf92685a388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation RMSE for LightGbm = 0.11961958993912819\n"
     ]
    }
   ],
   "source": [
    "light_gbm_predictions=np.zeros((len(test),1))\n",
    "y_train_pred_gbm=np.zeros((len(train),1))\n",
    "kfold=KFold(random_state=42)\n",
    "for idx,(train_idx,val_idx) in enumerate(kfold.split(train.values,y)):\n",
    "    x_train=final_pipeline.fit_transform(train.loc[train_idx])\n",
    "    x_val=final_pipeline.transform(train.loc[val_idx])\n",
    "    x_test=final_pipeline.transform(test)\n",
    "    model_lgb = lgm.LGBMRegressor(learning_rate=0.05,n_estimators=750,num_leaves=5,max_depth=3,\n",
    "                            objective=\"regression\",subsample=0.5,colsample_bytree=0.5)\n",
    "    model_lgb.fit(x_train,y[train_idx])\n",
    "    p=model_lgb.predict(x_val)\n",
    "    \n",
    "    y_train_pred_gbm[val_idx]+=np.reshape(p,(len(p),1))\n",
    "    p=model_lgb.predict(x_test)\n",
    "    light_gbm_predictions+=np.reshape(p,(len(p),1))/5\n",
    "print(\"Cross Validation RMSE for LightGbm =\",np.sqrt(mean_squared_error(y,y_train_pred_gbm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiBq4tBtW0Ce"
   },
   "source": [
    "**Support Vector Machines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-lq6vd1W262",
    "outputId": "64993c4e-deba-45ab-d55c-e016f391448f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11392404796806038\n",
      "{'C': 18}\n"
     ]
    }
   ],
   "source": [
    "GridSearch(SVR(gamma=0.0003,epsilon=0.001),{\"C\":[5,10,15,12,18]},X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kGvEzMMVW3Aq",
    "outputId": "21867f40-425f-4a81-9ded-c2f07a8867b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation RMSE for Support Vector Macines = 0.11579710917188761\n"
     ]
    }
   ],
   "source": [
    "svr_predictions=np.zeros((len(test),1))\n",
    "y_train_pred_svr=np.zeros((len(train),1))\n",
    "kfold=KFold(random_state=42)\n",
    "for idx,(train_idx,val_idx) in enumerate(kfold.split(train.values,y)):\n",
    "    x_train=final_pipeline.fit_transform(train.loc[train_idx])\n",
    "    x_val=final_pipeline.transform(train.loc[val_idx])\n",
    "    x_test=final_pipeline.transform(test)\n",
    "    svr = SVR(C=15,epsilon=0.001,gamma=0.0003)\n",
    "    svr.fit(x_train,y[train_idx])\n",
    "    p=svr.predict(x_val)\n",
    "    \n",
    "    y_train_pred_svr[val_idx]+=np.reshape(p,(len(p),1))\n",
    "    p=svr.predict(x_test)\n",
    "    svr_predictions+=np.reshape(p,(len(p),1))/5\n",
    "print(\"Cross Validation RMSE for Support Vector Macines =\",np.sqrt(mean_squared_error(y,y_train_pred_svr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOhVuE2glev8"
   },
   "source": [
    "**StackedCVRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeNQQUS4la88",
    "outputId": "110b3856-fb27-435e-f3e4-29be30e461a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation RMSE for Stacking Regressor = 0.11103321301954662\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(C=18,epsilon=0.001,gamma=0.0003)\n",
    "model_lgb = lgm.LGBMRegressor(learning_rate=0.05,n_estimators=750,num_leaves=5,max_depth=3,\n",
    "                            objective=\"regression\",subsample=0.5,colsample_bytree=0.5)\n",
    "ridge=Ridge(alpha=15)\n",
    "lasso=Lasso(alpha=0.0004)\n",
    "en=ElasticNet(alpha=0.001,l1_ratio=0.3)\n",
    "sr_predictions1=np.zeros((len(test),1))\n",
    "xgboost = XGBRegressor(objective='reg:squarederror',n_estimators=1000,learning_rate=0.01,max_depth=3,gamma=0.01,colsample_bytree=0.8,subsample=0.9)\n",
    "y_train_pred_sr1=np.zeros((len(train),1))\n",
    "kfold=KFold(random_state=42)\n",
    "for idx,(train_idx,val_idx) in enumerate(kfold.split(train.values,y)):\n",
    "    x_train=final_pipeline.fit_transform(train.loc[train_idx])\n",
    "    x_val=final_pipeline.transform(train.loc[val_idx])\n",
    "    x_test=final_pipeline.transform(test)\n",
    "    model=StackingRegressor(estimators=[(\"Support Vector Machines\",svr),(\"Ridge\",ridge),(\"LightGBM\",model_lgb)],final_estimator=LinearRegression())\n",
    "\n",
    "    model.fit(x_train,y[train_idx])\n",
    "    p=model.predict(x_val)\n",
    "    \n",
    "    y_train_pred_sr1[val_idx]+=np.reshape(p,(len(p),1))\n",
    "    p=model.predict(x_test)\n",
    "    sr_predictions1+=np.reshape(p,(len(p),1))/5\n",
    "print(\"Cross Validation RMSE for Stacking Regressor =\",np.sqrt(mean_squared_error(y,y_train_pred_sr1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Q_wAdM3yaFY",
    "outputId": "968c632f-ec1a-4af5-bd0a-813522909e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation RMSE for Stacking Regressor = 0.10964565767368856\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(C=18,epsilon=0.001,gamma=0.0003)\n",
    "model_lgb = lgm.LGBMRegressor(learning_rate=0.05,n_estimators=750,num_leaves=5,max_depth=3,\n",
    "                            objective=\"regression\",subsample=0.5,colsample_bytree=0.5)\n",
    "ridge=Ridge(alpha=16)\n",
    "lasso=Lasso(alpha=0.0004)\n",
    "en=ElasticNet(alpha=0.001,l1_ratio=0.3)\n",
    "sr_predictions2=np.zeros((len(test),1))\n",
    "xgboost = XGBRegressor(objective='reg:squarederror',n_estimators=1000,learning_rate=0.01,max_depth=3,gamma=0.01,colsample_bytree=0.8,subsample=0.9)\n",
    "y_train_pred_sr2=np.zeros((len(train),1))\n",
    "kfold=KFold(random_state=42)\n",
    "for idx,(train_idx,val_idx) in enumerate(kfold.split(train.values,y)):\n",
    "    x_train=final_pipeline.fit_transform(train.loc[train_idx])\n",
    "    x_val=final_pipeline.transform(train.loc[val_idx])\n",
    "    x_test=final_pipeline.transform(test)\n",
    "    model=StackingRegressor(estimators=[(\"Support Vector Machines\",svr),(\"Lasso\",lasso),(\"LightGBM\",model_lgb)],final_estimator=LinearRegression())\n",
    "\n",
    "    model.fit(x_train,y[train_idx])\n",
    "    p=model.predict(x_val)\n",
    "    \n",
    "    y_train_pred_sr2[val_idx]+=np.reshape(p,(len(p),1))\n",
    "    p=model.predict(x_test)\n",
    "    sr_predictions2+=np.reshape(p,(len(p),1))/5\n",
    "print(\"Cross Validation RMSE for Stacking Regressor =\",np.sqrt(mean_squared_error(y,y_train_pred_sr2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yg7S9b8iCD1U",
    "outputId": "88016a5e-e7ec-4c10-e300-7d61846ae001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation RMSE for Stacking Regressor = 0.10991730710799671\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(C=18,epsilon=0.001,gamma=0.0003)\n",
    "model_lgb = lgm.LGBMRegressor(learning_rate=0.05,n_estimators=750,num_leaves=5,max_depth=3,\n",
    "                            objective=\"regression\",subsample=0.5,colsample_bytree=0.5)\n",
    "ridge=Ridge(alpha=16)\n",
    "lasso=Lasso(alpha=0.0004)\n",
    "en=ElasticNet(alpha=0.001,l1_ratio=0.5)\n",
    "sr_predictions3=np.zeros((len(test),1))\n",
    "xgboost = XGBRegressor(objective='reg:squarederror',n_estimators=1000,learning_rate=0.01,max_depth=3,gamma=0.01,colsample_bytree=0.8,subsample=0.9)\n",
    "y_train_pred_sr3=np.zeros((len(train),1))\n",
    "kfold=KFold(random_state=42)\n",
    "for idx,(train_idx,val_idx) in enumerate(kfold.split(train.values,y)):\n",
    "    x_train=final_pipeline.fit_transform(train.loc[train_idx])\n",
    "    x_val=final_pipeline.transform(train.loc[val_idx])\n",
    "    x_test=final_pipeline.transform(test)\n",
    "    model=StackingRegressor(estimators=[(\"Support Vector Machines\",svr),(\"ElasticNet\",en),(\"LightGBM\",model_lgb)],final_estimator=LinearRegression())\n",
    "\n",
    "    model.fit(x_train,y[train_idx])\n",
    "    p=model.predict(x_val)\n",
    "    \n",
    "    y_train_pred_sr3[val_idx]+=np.reshape(p,(len(p),1))\n",
    "    p=model.predict(x_test)\n",
    "    sr_predictions3+=np.reshape(p,(len(p),1))/5\n",
    "print(\"Cross Validation RMSE for Stacking Regressor =\",np.sqrt(mean_squared_error(y,y_train_pred_sr3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "4E1_35vca9fe"
   },
   "outputs": [],
   "source": [
    "predictions=sr_predictions3\n",
    "predictions=np.expm1(predictions)\n",
    "predictions[1089]=mean_gridlivarea\n",
    "#predictions[523]=mean_masvnrarea\n",
    "#predictions[1130]=mean_masvnrarea\n",
    "#predictions[1026]=mean_openporch\n",
    "#predictions[1097]=mean_openporch\n",
    "create_submission(predictions,\"Lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2auqv3abA5j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Housing Prices Modelling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
